<!DOCTYPE html>
<!--
 | Generated by Apache Maven Doxia Site Renderer 1.8.1 from src/site/markdown/maven-usage_3.x.md at 2020-10-22
 | Rendered using Apache Maven Fluido Skin 1.7
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="Date-Revision-yyyymmdd" content="20201022" />
    <meta http-equiv="Content-Language" content="en" />
    <title>Hadoop Unit &#x2013; Maven usage</title>
    <link rel="stylesheet" href="./css/apache-maven-fluido-1.7.min.css" />
    <link rel="stylesheet" href="./css/site.css" />
    <link rel="stylesheet" href="./css/print.css" media="print" />
    <script type="text/javascript" src="./js/apache-maven-fluido-1.7.min.js"></script>
    <!-- Google Analytics -->
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-11955429-1']);
      _gaq.push(['_trackPageview']);
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
  </head>
  <body class="topBarEnabled">
    <a href="https://github.com/jetoile/hadoop-unit">
      <img style="position: absolute; top: 0; right: 0; border: 0; z-index: 10000;"
        src="https://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png"
        alt="Fork me on GitHub">
    </a>
    <div id="topbar" class="navbar navbar-fixed-top navbar-inverse">
      <div class="navbar-inner">
            <div class="container"><div class="nav-collapse">
            <ul class="nav">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Menu <b class="caret"></b></a>
        <ul class="dropdown-menu">
            <li><a href="index.html" title="Home">Home</a></li>
            <li><a href="what-is-hadoop-unit.html" title="What is Hadoop Unit">What is Hadoop Unit</a></li>
            <li><a href="why-hadoop-unit.html" title="Why Hadoop Unit">Why Hadoop Unit</a></li>
            <li class="dropdown-submenu">
<a href="" title="Installation">Installation</a>
              <ul class="dropdown-menu">
                  <li><a href="install-hadoop-unit-standalone.html" title="Install the standalone Hadoop Unit mode">Install the standalone Hadoop Unit mode</a></li>
                  <li><a href="maven-usage_2.x.html" title="Integrate Hadoop Unit 2.x in your maven project">Integrate Hadoop Unit 2.x in your maven project</a></li>
                  <li><a href="maven-usage_3.x.html" title="Integrate Hadoop Unit 3.x in your maven project">Integrate Hadoop Unit 3.x in your maven project</a></li>
              </ul>
            </li>
            <li><a href="cli.html" title="Use CLI to operate Hadoop Unit Standalone">Use CLI to operate Hadoop Unit Standalone</a></li>
            <li><a href="howto-integrationtest.html" title="How to write integration test with Hadoop Unit">How to write integration test with Hadoop Unit</a></li>
            <li><a href="why-hadoopunit-v3.html" title="Why Hadoop Unit v3">Why Hadoop Unit v3</a></li>
            <li><a href="plugin-development.html" title="Develop your own plugin">Develop your own plugin</a></li>
            <li><a href="howto-build.html" title="Howto build">Howto build</a></li>
            <li><a href="focus.html" title="Focus">Focus</a></li>
            <li><a href="faq.html" title="FAQ">FAQ</a></li>
            <li><a href="licence.html" title="Licence">Licence</a></li>
        </ul>
      </li>
            </ul>
<form id="search-form" action="https://www.google.com/search" method="get"  class="navbar-search pull-right" >
  <input value="$sitesearchValue" name="sitesearch" type="hidden"/>
  <input class="search-query" name="q" id="query" type="text" />
</form>
<script type="text/javascript">asyncJs( 'https://cse.google.com/brand?form=search-form' )</script>
            </div>
        </div>
      </div>
    </div>
    <div class="container">
      <div id="banner">
        <div class="pull-left"><div id="bannerLeft"><h2>hadoop-unit-site</h2>
</div>
</div>
        <div class="pull-right"></div>
        <div class="clear"><hr/></div>
      </div>

      <div id="breadcrumbs">
        <ul class="breadcrumb">
        <li id="publishDate">Last Published: 2020-10-22<span class="divider">|</span>
</li>
          <li id="projectVersion">Version: 3.8</li>
        </ul>
      </div>
        <div id="bodyColumn" >
<h1>Maven usage</h1>
<p>There are 3 modes to integrate Hadoop Unit in your maven build:</p>

<ul>
  
<li><a href="#simple-dependency-usage">simple dependency usage</a></li>
  
<li><a href="#use-the-maven-integration-plugin-in-mode-embedded">use the maven integration plugin in mode embedded</a></li>
  
<li><a href="#use-the-maven-integration-plugin-in-mode-remote">use the maven integration plugin in mode remote</a></li>
</ul>
<p>Note that the mode <b>maven integration plugin in mode embedded</b> is the most recommanded.</p>

<div id="simple-dependency-usage"></div>
<h1>Simple dependency Usage</h1>
<p>With maven, add dependencies of components which are needed.</p>
<p>Sample:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">&lt;dependency&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-unit-hdfs&lt;/artifactId&gt;
    &lt;version&gt;3.1&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</pre></div></div>
<p>In test do:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">@BeforeClass
public static void setup() {
    HadoopBootstrap.INSTANCE.startAll();
}

@AfterClass
public static void tearDown() {
    HadoopBootstrap.INSTANCE.stopAll();
}
</pre></div></div>

<blockquote>
<p><b>Note that because Hadoop Unit use, in this mode, the mecanism of maven dependency transitivity, if you just need, for example Hbase, you just need to indicate the dependency on the artifactId <tt>hadoop-unit-hbase</tt>. Automaticaly, Hadoop Unit will start Zookeeper and Hdfs under the hood and it will start then in the right order.</b></p>
</blockquote>
<p>It is also possible to indicate which components you need.</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">@BeforeClass
public static void setup() throws NotFoundServiceException {
    HadoopBootstrap.INSTANCE
        .add(Component.ZOOKEEPER)
        .add(Component.HDFS)
        .add(Component.HIVEMETA)
        .add(Component.HIVESERVER2)
        .startAll();
}

@AfterClass
public static void tearDown() throws NotFoundServiceException {
    HadoopBootstrap.INSTANCE
        .stopAll();
}
</pre></div></div>

<blockquote>
<p><b>Note that, in this mode, you could have classpath issues (with guava for example) with yours tests. Indeed, because Hadoop Unit has a lot of dependencies (hadoop&#x2019;s hells), you may have stuff like <tt>NoSuchMethodError</tt> or <tt>NoClassDefFound</tt>. To fix these issues, use the maven&#x2019;s exclusion option.</b></p>
</blockquote>

<div id="use-the-maven-integration-plugin-in-mode-embedded"></div>
<h1>Use the maven integration plugin in mode embedded</h1>

<blockquote>
<p><b>This is the recommended mode</b></p>
</blockquote>
<p>To use it, add into the pom project stuff like that:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;

     &lt;dependency&gt;
        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-unit-client-hdfs&lt;/artifactId&gt;
        &lt;version&gt;2.10&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-unit-client-hive&lt;/artifactId&gt;
        &lt;version&gt;2.10&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-unit-client-spark&lt;/artifactId&gt;
        &lt;version&gt;2.10&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;excludes&gt;
                    &lt;exclude&gt;**/*IntegrationTest.java&lt;/exclude&gt;
                &lt;/excludes&gt;
            &lt;/configuration&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;integration-test&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;test&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;phase&gt;integration-test&lt;/phase&gt;
                    &lt;configuration&gt;
                        &lt;excludes&gt;
                            &lt;exclude&gt;none&lt;/exclude&gt;
                        &lt;/excludes&gt;
                        &lt;includes&gt;
                            &lt;include&gt;**/*IntegrationTest.java&lt;/include&gt;
                        &lt;/includes&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
            &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
            &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;start&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;embedded-start&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;phase&gt;pre-integration-test&lt;/phase&gt;
                &lt;/execution&gt;
                &lt;execution&gt;
                    &lt;id&gt;embedded-stop&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;embedded-stop&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;phase&gt;post-integration-test&lt;/phase&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
            &lt;configuration&gt;
                &lt;components&gt;
                    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
                        &lt;componentName&gt;ZOOKEEPER&lt;/componentName&gt;
                        &lt;artifactId&gt;hadoop-unit-zookeeper&lt;/artifactId&gt;
                        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
                        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
                        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.ZookeeperBootstrap&lt;/mainClass&gt;
                    &lt;/componentArtifact&gt;
                    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
                        &lt;componentName&gt;HDFS&lt;/componentName&gt;
                        &lt;artifactId&gt;hadoop-unit-hdfs&lt;/artifactId&gt;
                        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
                        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
                        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.HdfsBootstrap&lt;/mainClass&gt;
                    &lt;/componentArtifact&gt;
                    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
                        &lt;componentName&gt;HIVEMETA&lt;/componentName&gt;
                        &lt;artifactId&gt;hadoop-unit-hive&lt;/artifactId&gt;
                        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
                        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
                        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.HiveMetastoreBootstrap&lt;/mainClass&gt;
                    &lt;/componentArtifact&gt;
                    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
                        &lt;componentName&gt;HIVESERVER2&lt;/componentName&gt;
                        &lt;artifactId&gt;hadoop-unit-hive&lt;/artifactId&gt;
                        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
                        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
                        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.HiveServer2Bootstrap&lt;/mainClass&gt;
                    &lt;/componentArtifact&gt;
                    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
                        &lt;componentName&gt;SOLRCLOUD&lt;/componentName&gt;
                        &lt;artifactId&gt;hadoop-unit-solrcloud&lt;/artifactId&gt;
                        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
                        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
                        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.SolrCloudBootstrap&lt;/mainClass&gt;
                        &lt;properties&gt;
                            &lt;solr.dir&gt;file://${project.basedir}/src/test/resources/solr&lt;/solr.dir&gt;
                        &lt;/properties&gt;
                    &lt;/componentArtifact&gt;
                &lt;/components&gt;

            &lt;/configuration&gt;

        &lt;/plugin&gt;

    &lt;/plugins&gt;
&lt;/build&gt;
</pre></div></div>
<p>Values can be:</p>

<ul>
  
<li>HDFS</li>
  
<li>ZOOKEEPER</li>
  
<li>HIVEMETA</li>
  
<li>HIVESERVER2</li>
  
<li>SOLR</li>
  
<li>SOLRCLOUD</li>
  
<li>OOZIE</li>
  
<li>KAFKA</li>
  
<li>CONFLUENT_KAFKA</li>
  
<li>CONFLUENT_SCHEMAREGISTRY</li>
  
<li>CONFLUENT_KAFKA_REST</li>
  
<li>CONFLUENT_KSQL_REST</li>
  
<li>HBASE</li>
  
<li>MONGODB</li>
  
<li>CASSANDRA</li>
  
<li>ELASTICSEARCH</li>
  
<li>NEO4J</li>
  
<li>KNOX</li>
  
<li>ALLUXIO</li>
  
<li>REDIS</li>
</ul>
<p>It is also possible to override configurations with a list of <tt>properties</tt> which accept a map (ie. <tt>&lt;key&gt;value&lt;/key&gt;</tt> and where <tt>key</tt> is a property from the file <tt>hadoop-unit-default.properties</tt>).</p>
<p>For solrcloud, it is mandatory to indicate where is the solr config:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">    &lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
        &lt;componentName&gt;SOLRCLOUD&lt;/componentName&gt;
        &lt;artifactId&gt;hadoop-unit-solrcloud&lt;/artifactId&gt;
        &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
        &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
        &lt;mainClass&gt;fr.jetoile.hadoopunit.component.SolrCloudBootstrap&lt;/mainClass&gt;
        &lt;properties&gt;
            &lt;solr.dir&gt;file://${project.basedir}/src/test/resources/solr&lt;/solr.dir&gt;
        &lt;/properties&gt;
    &lt;/componentArtifact&gt;
</pre></div></div>
<p>Here is a sample integration test:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">public class HdfsBootstrapIntegrationTest {

    static private Configuration configuration;


    @BeforeClass
    public static void setup() throws BootstrapException {
        try {
            configuration = new PropertiesConfiguration(&quot;hadoop-unit-default.properties&quot;);
        } catch (ConfigurationException e) {
            throw new BootstrapException(&quot;bad config&quot;, e);
        }
    }


    @Test
    public void hdfsShouldStart() throws Exception {

        FileSystem hdfsFsHandle = HdfsUtils.INSTANCE.getFileSystem();


        FSDataOutputStream writer = hdfsFsHandle.create(new Path(configuration.getString(HadoopUnitConfig.HDFS_TEST_FILE_KEY)));
        writer.writeUTF(configuration.getString(HadoopUnitConfig.HDFS_TEST_STRING_KEY));
        writer.close();

        // Read the file and compare to test string
        FSDataInputStream reader = hdfsFsHandle.open(new Path(configuration.getString(HadoopUnitConfig.HDFS_TEST_FILE_KEY)));
        assertEquals(reader.readUTF(), configuration.getString(HadoopUnitConfig.HDFS_TEST_STRING_KEY));
        reader.close();
        hdfsFsHandle.close();

        URL url = new URL(
                String.format( &quot;http://localhost:%s/webhdfs/v1?op=GETHOMEDIRECTORY&amp;user.name=guest&quot;,
                        configuration.getInt( HadoopUnitConfig.HDFS_NAMENODE_HTTP_PORT_KEY ) ) );
        URLConnection connection = url.openConnection();
        connection.setRequestProperty( &quot;Accept-Charset&quot;, &quot;UTF-8&quot; );
        BufferedReader response = new BufferedReader( new InputStreamReader( connection.getInputStream() ) );
        String line = response.readLine();
        response.close();
        assertThat(&quot;{\&quot;Path\&quot;:\&quot;/user/guest\&quot;}&quot;).isEqualTo(line);
    }
}
</pre></div></div>
<div class="section">
<h2><a name="For_windows_user"></a>For windows user</h2>
<p>If you get errors which tell you that you can not write in the directory <tt>C:\tmp</tt> or <tt>D:\tmp</tt>, it is because you are not admin of you laptop. If you can create this directory and give access to it. If not possible, edit your <tt>pom</tt> file and for your components, set the property to the right value:</p>
<p>sample:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">&lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
    &lt;componentName&gt;ZOOKEEPER&lt;/componentName&gt;
    &lt;artifactId&gt;hadoop-unit-zookeeper&lt;/artifactId&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
    &lt;mainClass&gt;fr.jetoile.hadoopunit.component.ZookeeperBootstrap&lt;/mainClass&gt;
    &lt;properties&gt;
      &lt;zookeeper.temp.dir&gt;C:/&lt;path where you can write&gt;/tmp/embedded_zk&lt;/zookeeper.temp.dir&gt;
    &lt;/properties&gt;
&lt;/componentArtifact&gt;
&lt;componentArtifact implementation=&quot;fr.jetoile.hadoopunit.ComponentArtifact&quot;&gt;
    &lt;componentName&gt;HDFS&lt;/componentName&gt;
    &lt;artifactId&gt;hadoop-unit-hdfs&lt;/artifactId&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
    &lt;mainClass&gt;fr.jetoile.hadoopunit.component.HdfsBootstrap&lt;/mainClass&gt;
    &lt;properties&gt;
      &lt;hdfs.temp.dir&gt;C:/&lt;path where you can write&gt;/tmp/embedded_hdfs&lt;/hdfs.temp.dir&gt;
    &lt;/properties&gt;
&lt;/componentArtifact&gt;
</pre></div></div>

<div id="use-the-maven-integration-plugin-in-mode-remote"></div>
<h1>Use the maven integration plugin in mode remote</h1>
<p>This plugin start/stop a remote local hadoop-unit-standalone.</p>
<p>To use it, add into the pom project stuff like that:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;excludes&gt;
            &lt;exclude&gt;**/*IntegrationTest.java&lt;/exclude&gt;
        &lt;/excludes&gt;
    &lt;/configuration&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;integration-test&lt;/id&gt;
            &lt;goals&gt;
                &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;phase&gt;integration-test&lt;/phase&gt;
            &lt;configuration&gt;
                &lt;excludes&gt;
                    &lt;exclude&gt;none&lt;/exclude&gt;
                &lt;/excludes&gt;
                &lt;includes&gt;
                    &lt;include&gt;**/*IntegrationTest.java&lt;/include&gt;
                &lt;/includes&gt;
            &lt;/configuration&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;

&lt;plugin&gt;
    &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;version&gt;2.10&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;start&lt;/id&gt;
            &lt;goals&gt;
                &lt;goal&gt;start&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;phase&gt;pre-integration-test&lt;/phase&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
        &lt;hadoopUnitPath&gt;/home/khanh/tools/hadoop-unit-standalone&lt;/hadoopUnitPath&gt;
        &lt;exec&gt;./hadoop-unit-standalone&lt;/exec&gt;
        &lt;values&gt;
            &lt;value&gt;ZOOKEEPER&lt;/value&gt;
            &lt;value&gt;HDFS&lt;/value&gt;
            &lt;value&gt;HIVEMETA&lt;/value&gt;
            &lt;value&gt;HIVESERVER2&lt;/value&gt;
        &lt;/values&gt;
        &lt;outputFile&gt;/tmp/toto.txt&lt;/outputFile&gt;
    &lt;/configuration&gt;

&lt;/plugin&gt;

&lt;plugin&gt;
    &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;version&gt;2.10&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;stop&lt;/id&gt;
            &lt;goals&gt;
                &lt;goal&gt;stop&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;phase&gt;post-integration-test&lt;/phase&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
        &lt;hadoopUnitPath&gt;/home/khanh/tools/hadoop-unit-standalone&lt;/hadoopUnitPath&gt;
        &lt;exec&gt;./hadoop-unit-standalone&lt;/exec&gt;
        &lt;outputFile&gt;/tmp/toto.txt&lt;/outputFile&gt;
    &lt;/configuration&gt;

&lt;/plugin&gt;
</pre></div></div>
<p>Values can be:</p>

<ul>
  
<li>HDFS</li>
  
<li>ZOOKEEPER</li>
  
<li>HIVEMETA</li>
  
<li>HIVESERVER2</li>
  
<li>SOLR</li>
  
<li>SOLRCLOUD</li>
  
<li>OOZIE</li>
  
<li>KAFKA</li>
  
<li>CONFLUENT_KAFKA</li>
  
<li>CONFLUENT_SCHEMAREGISTRY</li>
  
<li>CONFLUENT_KAFKA_REST</li>
  
<li>CONFLUENT_KSQL_REST</li>
  
<li>HBASE</li>
  
<li>MONGODB</li>
  
<li>CASSANDRA</li>
  
<li>ELASTICSEARCH</li>
  
<li>NEO4J</li>
  
<li>KNOX</li>
  
<li>ALLUXIO</li>
  
<li>REDIS</li>
</ul>
<p><tt>hadoopUnitPath</tt> is not mandatory but system enviroment variable <tt>HADOOP_UNIT_HOME</tt> must be defined.</p>
<p><tt>exec</tt> variable is optional.</p>
<p>If both are set, <tt>HADOOP_UNIT_HOME</tt> override <tt>hadoopUnitPath</tt>.</p>
<p><i>Warning: This plugin will modify hadoop.properties and delete hadoop unit logs.</i></p>
<p>Here is a sample integration test:</p>

<div class="source">
<div class="source"><pre class="prettyprint linenums">public class HdfsBootstrapIntegrationTest {

    static private Configuration configuration;


    @BeforeClass
    public static void setup() throws BootstrapException {
        try {
            configuration = new PropertiesConfiguration(&quot;hadoop-unit-default.properties&quot;);
        } catch (ConfigurationException e) {
            throw new BootstrapException(&quot;bad config&quot;, e);
        }
    }


    @Test
    public void hdfsShouldStart() throws Exception {

        FileSystem hdfsFsHandle = HdfsUtils.INSTANCE.getFileSystem();


        FSDataOutputStream writer = hdfsFsHandle.create(new Path(configuration.getString(HadoopUnitConfig.HDFS_TEST_FILE_KEY)));
        writer.writeUTF(configuration.getString(HadoopUnitConfig.HDFS_TEST_STRING_KEY));
        writer.close();

        // Read the file and compare to test string
        FSDataInputStream reader = hdfsFsHandle.open(new Path(configuration.getString(HadoopUnitConfig.HDFS_TEST_FILE_KEY)));
        assertEquals(reader.readUTF(), configuration.getString(HadoopUnitConfig.HDFS_TEST_STRING_KEY));
        reader.close();
        hdfsFsHandle.close();

        URL url = new URL(
                String.format( &quot;http://localhost:%s/webhdfs/v1?op=GETHOMEDIRECTORY&amp;user.name=guest&quot;,
                        configuration.getInt( HadoopUnitConfig.HDFS_NAMENODE_HTTP_PORT_KEY ) ) );
        URLConnection connection = url.openConnection();
        connection.setRequestProperty( &quot;Accept-Charset&quot;, &quot;UTF-8&quot; );
        BufferedReader response = new BufferedReader( new InputStreamReader( connection.getInputStream() ) );
        String line = response.readLine();
        response.close();
        assertThat(&quot;{\&quot;Path\&quot;:\&quot;/user/guest\&quot;}&quot;).isEqualTo(line);
    }
}
</pre></div></div></div>
        </div>
    </div>
    <hr/>
    <footer>
      <div class="container">
        <div class="row">
            <p>Copyright &copy;2020.
All rights reserved.</p>
        </div>
        <p id="poweredBy" class="pull-right"><a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy"><img class="builtBy" alt="Built by Maven" src="./images/logos/maven-feather.png" /></a>
</p>
      </div>
    </footer>
  </body>
</html>
